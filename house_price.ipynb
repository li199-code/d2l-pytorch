{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1l4vgIP_aUbkklZusBG2nu4gKbubto1Jo",
      "authorship_tag": "ABX9TyPExQa+6Hb34sRDq+MpbwEz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/li199-code/d2l-pytorch/blob/main/house_price.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vU-eKEC7k5BY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8c45914-8c36-4213-e7f1-960be4118ce3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cpu device\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch import nn\n",
        "\n",
        "device = (\n",
        "    \"cuda\"\n",
        "    if torch.cuda.is_available()\n",
        "    else \"cpu\"\n",
        ")\n",
        "print(f\"Using {device} device\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "lr=5\n",
        "epochs = 100"
      ],
      "metadata": {
        "id": "eQ4j7EcC9Hpm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data=pd.read_csv('/content/drive/MyDrive/data/kaggle_house_pred_train.csv')\n",
        "test_data=pd.read_csv('/content/drive/MyDrive/data/kaggle_house_pred_test.csv')"
      ],
      "metadata": {
        "id": "8u1_DNKRlIlY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_features = pd.concat((train_data.iloc[:, 1:-1], test_data.iloc[:, 1:]))\n",
        "# print(len(all_features))\n",
        "# print(all_features.shape)"
      ],
      "metadata": {
        "id": "DCI0sVDcmZAe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 若无法获得测试数据，则可根据训练数据计算均值和标准差\n",
        "numeric_features = all_features.dtypes[all_features.dtypes != 'object'].index\n",
        "all_features[numeric_features] = all_features[numeric_features].apply(\n",
        "    lambda x: (x - x.mean()) / (x.std()))\n",
        "all_features[numeric_features] = all_features[numeric_features].fillna(0)"
      ],
      "metadata": {
        "id": "Fhq6Bi2MnSiw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 文本数据的处理：one-hot encoding\n",
        "all_features = pd.get_dummies(all_features, dummy_na=True)"
      ],
      "metadata": {
        "id": "vkRF7i4stHXF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_train = train_data.shape[0]\n",
        "train_features = torch.tensor(all_features[:n_train].values, dtype=torch.float32)\n",
        "test_features = torch.tensor(all_features[n_train:].values, dtype=torch.float32)\n",
        "train_labels = torch.tensor(\n",
        "    train_data.SalePrice.values.reshape(-1, 1), dtype=torch.float32)"
      ],
      "metadata": {
        "id": "ug03kzpGBdVJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_array(data, batch_size, train=True):\n",
        "  dataset = torch.utils.data.TensorDataset(*data)\n",
        "  return DataLoader(dataset, batch_size, shuffle=train)\n",
        "\n",
        "\n",
        "trainloader = load_array((train_features, train_labels), batch_size)\n"
      ],
      "metadata": {
        "id": "cAS6be65DSBJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "in_features = train_features.shape[1]\n",
        "net = nn.Sequential(\n",
        "  nn.Linear(in_features, 512),\n",
        "  nn.ReLU(),\n",
        "  nn.Dropout(0.2),\n",
        "  nn.Linear(512, 1),\n",
        ")\n",
        "\n",
        "model = net.to(device)"
      ],
      "metadata": {
        "id": "kXWSIMgk0nP6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = nn.MSELoss()\n",
        "def log_rmse(net, features, labels):\n",
        "    # 为了在取对数时进一步稳定该值，将小于1的值设置为1\n",
        "    clipped_preds = torch.clamp(net(features), 1, float('inf'))\n",
        "    rmse = torch.sqrt(loss_fn(torch.log(clipped_preds),\n",
        "                           torch.log(labels)))\n",
        "    return rmse.item()\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
      ],
      "metadata": {
        "id": "BJF7MLDE8p1W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(trainloader, model, loss_fn, optimizer):\n",
        "  size = n_train\n",
        "  model.train()\n",
        "  for batch, (x, y) in enumerate(trainloader):\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    pred = model(x)\n",
        "    l = loss_fn(pred, y)\n",
        "    l.backward()\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    if batch%10==0:\n",
        "      loss_num, current = l.item(), (batch + 1) * len(x)\n",
        "      print(f\"loss: {loss_num:>7f}  [{current:>5d}/{size:>5d}]\")\n"
      ],
      "metadata": {
        "id": "YAkMmkeL9pZi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for t in range(epochs):\n",
        "  print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "  train(trainloader, model, loss_fn, optimizer)\n",
        "\n",
        "torch.save(model, '/content/drive/MyDrive/checkpoint/model.pth')\n",
        "print('Done!')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w0oStb_pBl9I",
        "outputId": "49df80d5-5d16-402b-d89a-4662d0e22cf0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 48022446080.000000  [   64/ 1460]\n",
            "loss: 15442082816.000000  [  704/ 1460]\n",
            "loss: 5581964288.000000  [ 1344/ 1460]\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 5643421696.000000  [   64/ 1460]\n",
            "loss: 1503381632.000000  [  704/ 1460]\n",
            "loss: 1047202304.000000  [ 1344/ 1460]\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 1267903616.000000  [   64/ 1460]\n",
            "loss: 3767652608.000000  [  704/ 1460]\n",
            "loss: 2014125824.000000  [ 1344/ 1460]\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 2221456384.000000  [   64/ 1460]\n",
            "loss: 1353030144.000000  [  704/ 1460]\n",
            "loss: 486593600.000000  [ 1344/ 1460]\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 1240350336.000000  [   64/ 1460]\n",
            "loss: 1069307648.000000  [  704/ 1460]\n",
            "loss: 672020224.000000  [ 1344/ 1460]\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 1280180224.000000  [   64/ 1460]\n",
            "loss: 1072667392.000000  [  704/ 1460]\n",
            "loss: 1244533248.000000  [ 1344/ 1460]\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 2614052352.000000  [   64/ 1460]\n",
            "loss: 6460640768.000000  [  704/ 1460]\n",
            "loss: 918502400.000000  [ 1344/ 1460]\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss: 933940992.000000  [   64/ 1460]\n",
            "loss: 1048929024.000000  [  704/ 1460]\n",
            "loss: 3078488064.000000  [ 1344/ 1460]\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss: 4065095680.000000  [   64/ 1460]\n",
            "loss: 4726143488.000000  [  704/ 1460]\n",
            "loss: 1202056576.000000  [ 1344/ 1460]\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss: 942168064.000000  [   64/ 1460]\n",
            "loss: 1169808000.000000  [  704/ 1460]\n",
            "loss: 613264640.000000  [ 1344/ 1460]\n",
            "Epoch 11\n",
            "-------------------------------\n",
            "loss: 1000544576.000000  [   64/ 1460]\n",
            "loss: 1299302400.000000  [  704/ 1460]\n",
            "loss: 1433208192.000000  [ 1344/ 1460]\n",
            "Epoch 12\n",
            "-------------------------------\n",
            "loss: 3430188032.000000  [   64/ 1460]\n",
            "loss: 1390732800.000000  [  704/ 1460]\n",
            "loss: 1090111232.000000  [ 1344/ 1460]\n",
            "Epoch 13\n",
            "-------------------------------\n",
            "loss: 1020606016.000000  [   64/ 1460]\n",
            "loss: 1218415360.000000  [  704/ 1460]\n",
            "loss: 1064997056.000000  [ 1344/ 1460]\n",
            "Epoch 14\n",
            "-------------------------------\n",
            "loss: 2169595392.000000  [   64/ 1460]\n",
            "loss: 750346688.000000  [  704/ 1460]\n",
            "loss: 624494400.000000  [ 1344/ 1460]\n",
            "Epoch 15\n",
            "-------------------------------\n",
            "loss: 710826496.000000  [   64/ 1460]\n",
            "loss: 938365312.000000  [  704/ 1460]\n",
            "loss: 1340528256.000000  [ 1344/ 1460]\n",
            "Epoch 16\n",
            "-------------------------------\n",
            "loss: 1125174400.000000  [   64/ 1460]\n",
            "loss: 649346496.000000  [  704/ 1460]\n",
            "loss: 1130088448.000000  [ 1344/ 1460]\n",
            "Epoch 17\n",
            "-------------------------------\n",
            "loss: 732944768.000000  [   64/ 1460]\n",
            "loss: 1047068800.000000  [  704/ 1460]\n",
            "loss: 1872571136.000000  [ 1344/ 1460]\n",
            "Epoch 18\n",
            "-------------------------------\n",
            "loss: 1362795392.000000  [   64/ 1460]\n",
            "loss: 1344333184.000000  [  704/ 1460]\n",
            "loss: 1078635136.000000  [ 1344/ 1460]\n",
            "Epoch 19\n",
            "-------------------------------\n",
            "loss: 878974464.000000  [   64/ 1460]\n",
            "loss: 888389440.000000  [  704/ 1460]\n",
            "loss: 935700864.000000  [ 1344/ 1460]\n",
            "Epoch 20\n",
            "-------------------------------\n",
            "loss: 1890240768.000000  [   64/ 1460]\n",
            "loss: 1254817920.000000  [  704/ 1460]\n",
            "loss: 605272128.000000  [ 1344/ 1460]\n",
            "Epoch 21\n",
            "-------------------------------\n",
            "loss: 614569344.000000  [   64/ 1460]\n",
            "loss: 1503030784.000000  [  704/ 1460]\n",
            "loss: 1560698112.000000  [ 1344/ 1460]\n",
            "Epoch 22\n",
            "-------------------------------\n",
            "loss: 1493366912.000000  [   64/ 1460]\n",
            "loss: 1852273152.000000  [  704/ 1460]\n",
            "loss: 757165120.000000  [ 1344/ 1460]\n",
            "Epoch 23\n",
            "-------------------------------\n",
            "loss: 1470280320.000000  [   64/ 1460]\n",
            "loss: 14507440128.000000  [  704/ 1460]\n",
            "loss: 1407843968.000000  [ 1344/ 1460]\n",
            "Epoch 24\n",
            "-------------------------------\n",
            "loss: 2163963904.000000  [   64/ 1460]\n",
            "loss: 2461541888.000000  [  704/ 1460]\n",
            "loss: 1848157824.000000  [ 1344/ 1460]\n",
            "Epoch 25\n",
            "-------------------------------\n",
            "loss: 1893340544.000000  [   64/ 1460]\n",
            "loss: 814171648.000000  [  704/ 1460]\n",
            "loss: 1174888704.000000  [ 1344/ 1460]\n",
            "Epoch 26\n",
            "-------------------------------\n",
            "loss: 1104780032.000000  [   64/ 1460]\n",
            "loss: 936667456.000000  [  704/ 1460]\n",
            "loss: 1281923328.000000  [ 1344/ 1460]\n",
            "Epoch 27\n",
            "-------------------------------\n",
            "loss: 888482880.000000  [   64/ 1460]\n",
            "loss: 11540760576.000000  [  704/ 1460]\n",
            "loss: 1923214080.000000  [ 1344/ 1460]\n",
            "Epoch 28\n",
            "-------------------------------\n",
            "loss: 1503721216.000000  [   64/ 1460]\n",
            "loss: 1100078592.000000  [  704/ 1460]\n",
            "loss: 1119618304.000000  [ 1344/ 1460]\n",
            "Epoch 29\n",
            "-------------------------------\n",
            "loss: 2110648320.000000  [   64/ 1460]\n",
            "loss: 3332792576.000000  [  704/ 1460]\n",
            "loss: 1388653184.000000  [ 1344/ 1460]\n",
            "Epoch 30\n",
            "-------------------------------\n",
            "loss: 915785856.000000  [   64/ 1460]\n",
            "loss: 993371520.000000  [  704/ 1460]\n",
            "loss: 1056653056.000000  [ 1344/ 1460]\n",
            "Epoch 31\n",
            "-------------------------------\n",
            "loss: 616962624.000000  [   64/ 1460]\n",
            "loss: 1096565504.000000  [  704/ 1460]\n",
            "loss: 1786899200.000000  [ 1344/ 1460]\n",
            "Epoch 32\n",
            "-------------------------------\n",
            "loss: 462370048.000000  [   64/ 1460]\n",
            "loss: 1131844224.000000  [  704/ 1460]\n",
            "loss: 1209372288.000000  [ 1344/ 1460]\n",
            "Epoch 33\n",
            "-------------------------------\n",
            "loss: 1091908096.000000  [   64/ 1460]\n",
            "loss: 1683543040.000000  [  704/ 1460]\n",
            "loss: 994482432.000000  [ 1344/ 1460]\n",
            "Epoch 34\n",
            "-------------------------------\n",
            "loss: 842490112.000000  [   64/ 1460]\n",
            "loss: 962294016.000000  [  704/ 1460]\n",
            "loss: 871844480.000000  [ 1344/ 1460]\n",
            "Epoch 35\n",
            "-------------------------------\n",
            "loss: 784755328.000000  [   64/ 1460]\n",
            "loss: 771326720.000000  [  704/ 1460]\n",
            "loss: 1162219008.000000  [ 1344/ 1460]\n",
            "Epoch 36\n",
            "-------------------------------\n",
            "loss: 635084032.000000  [   64/ 1460]\n",
            "loss: 588792448.000000  [  704/ 1460]\n",
            "loss: 739192768.000000  [ 1344/ 1460]\n",
            "Epoch 37\n",
            "-------------------------------\n",
            "loss: 618321472.000000  [   64/ 1460]\n",
            "loss: 660801472.000000  [  704/ 1460]\n",
            "loss: 1143659008.000000  [ 1344/ 1460]\n",
            "Epoch 38\n",
            "-------------------------------\n",
            "loss: 677380544.000000  [   64/ 1460]\n",
            "loss: 1020605440.000000  [  704/ 1460]\n",
            "loss: 727242240.000000  [ 1344/ 1460]\n",
            "Epoch 39\n",
            "-------------------------------\n",
            "loss: 636649600.000000  [   64/ 1460]\n",
            "loss: 2461298688.000000  [  704/ 1460]\n",
            "loss: 1017922560.000000  [ 1344/ 1460]\n",
            "Epoch 40\n",
            "-------------------------------\n",
            "loss: 1372914688.000000  [   64/ 1460]\n",
            "loss: 1288681216.000000  [  704/ 1460]\n",
            "loss: 1160783744.000000  [ 1344/ 1460]\n",
            "Epoch 41\n",
            "-------------------------------\n",
            "loss: 998734912.000000  [   64/ 1460]\n",
            "loss: 809041856.000000  [  704/ 1460]\n",
            "loss: 1075572096.000000  [ 1344/ 1460]\n",
            "Epoch 42\n",
            "-------------------------------\n",
            "loss: 684468864.000000  [   64/ 1460]\n",
            "loss: 871737728.000000  [  704/ 1460]\n",
            "loss: 533222496.000000  [ 1344/ 1460]\n",
            "Epoch 43\n",
            "-------------------------------\n",
            "loss: 840051392.000000  [   64/ 1460]\n",
            "loss: 1041000384.000000  [  704/ 1460]\n",
            "loss: 634289792.000000  [ 1344/ 1460]\n",
            "Epoch 44\n",
            "-------------------------------\n",
            "loss: 1144872320.000000  [   64/ 1460]\n",
            "loss: 978955136.000000  [  704/ 1460]\n",
            "loss: 982444160.000000  [ 1344/ 1460]\n",
            "Epoch 45\n",
            "-------------------------------\n",
            "loss: 1013338560.000000  [   64/ 1460]\n",
            "loss: 690578880.000000  [  704/ 1460]\n",
            "loss: 1288345856.000000  [ 1344/ 1460]\n",
            "Epoch 46\n",
            "-------------------------------\n",
            "loss: 722112704.000000  [   64/ 1460]\n",
            "loss: 746188672.000000  [  704/ 1460]\n",
            "loss: 429792640.000000  [ 1344/ 1460]\n",
            "Epoch 47\n",
            "-------------------------------\n",
            "loss: 1007965312.000000  [   64/ 1460]\n",
            "loss: 7317177344.000000  [  704/ 1460]\n",
            "loss: 1747172480.000000  [ 1344/ 1460]\n",
            "Epoch 48\n",
            "-------------------------------\n",
            "loss: 1426555648.000000  [   64/ 1460]\n",
            "loss: 1433899776.000000  [  704/ 1460]\n",
            "loss: 1262544256.000000  [ 1344/ 1460]\n",
            "Epoch 49\n",
            "-------------------------------\n",
            "loss: 787826816.000000  [   64/ 1460]\n",
            "loss: 945923328.000000  [  704/ 1460]\n",
            "loss: 1458449664.000000  [ 1344/ 1460]\n",
            "Epoch 50\n",
            "-------------------------------\n",
            "loss: 1195076352.000000  [   64/ 1460]\n",
            "loss: 3161637632.000000  [  704/ 1460]\n",
            "loss: 1801205504.000000  [ 1344/ 1460]\n",
            "Epoch 51\n",
            "-------------------------------\n",
            "loss: 889684096.000000  [   64/ 1460]\n",
            "loss: 1135967616.000000  [  704/ 1460]\n",
            "loss: 1691270656.000000  [ 1344/ 1460]\n",
            "Epoch 52\n",
            "-------------------------------\n",
            "loss: 1568353024.000000  [   64/ 1460]\n",
            "loss: 1401428992.000000  [  704/ 1460]\n",
            "loss: 1513513728.000000  [ 1344/ 1460]\n",
            "Epoch 53\n",
            "-------------------------------\n",
            "loss: 2093586688.000000  [   64/ 1460]\n",
            "loss: 1005106880.000000  [  704/ 1460]\n",
            "loss: 756355584.000000  [ 1344/ 1460]\n",
            "Epoch 54\n",
            "-------------------------------\n",
            "loss: 1230495488.000000  [   64/ 1460]\n",
            "loss: 741923264.000000  [  704/ 1460]\n",
            "loss: 917209984.000000  [ 1344/ 1460]\n",
            "Epoch 55\n",
            "-------------------------------\n",
            "loss: 897530752.000000  [   64/ 1460]\n",
            "loss: 1079202816.000000  [  704/ 1460]\n",
            "loss: 2036690176.000000  [ 1344/ 1460]\n",
            "Epoch 56\n",
            "-------------------------------\n",
            "loss: 1424323968.000000  [   64/ 1460]\n",
            "loss: 1158323584.000000  [  704/ 1460]\n",
            "loss: 587758400.000000  [ 1344/ 1460]\n",
            "Epoch 57\n",
            "-------------------------------\n",
            "loss: 1084156928.000000  [   64/ 1460]\n",
            "loss: 1095862400.000000  [  704/ 1460]\n",
            "loss: 1331984512.000000  [ 1344/ 1460]\n",
            "Epoch 58\n",
            "-------------------------------\n",
            "loss: 1477732224.000000  [   64/ 1460]\n",
            "loss: 845734592.000000  [  704/ 1460]\n",
            "loss: 1988934528.000000  [ 1344/ 1460]\n",
            "Epoch 59\n",
            "-------------------------------\n",
            "loss: 801818752.000000  [   64/ 1460]\n",
            "loss: 970445248.000000  [  704/ 1460]\n",
            "loss: 1206578176.000000  [ 1344/ 1460]\n",
            "Epoch 60\n",
            "-------------------------------\n",
            "loss: 921080064.000000  [   64/ 1460]\n",
            "loss: 1778988800.000000  [  704/ 1460]\n",
            "loss: 994381184.000000  [ 1344/ 1460]\n",
            "Epoch 61\n",
            "-------------------------------\n",
            "loss: 632283968.000000  [   64/ 1460]\n",
            "loss: 918125120.000000  [  704/ 1460]\n",
            "loss: 810368256.000000  [ 1344/ 1460]\n",
            "Epoch 62\n",
            "-------------------------------\n",
            "loss: 956834944.000000  [   64/ 1460]\n",
            "loss: 781142912.000000  [  704/ 1460]\n",
            "loss: 1255511168.000000  [ 1344/ 1460]\n",
            "Epoch 63\n",
            "-------------------------------\n",
            "loss: 479272512.000000  [   64/ 1460]\n",
            "loss: 729896704.000000  [  704/ 1460]\n",
            "loss: 758326464.000000  [ 1344/ 1460]\n",
            "Epoch 64\n",
            "-------------------------------\n",
            "loss: 810699904.000000  [   64/ 1460]\n",
            "loss: 1720109824.000000  [  704/ 1460]\n",
            "loss: 663162688.000000  [ 1344/ 1460]\n",
            "Epoch 65\n",
            "-------------------------------\n",
            "loss: 1217521536.000000  [   64/ 1460]\n",
            "loss: 1256717440.000000  [  704/ 1460]\n",
            "loss: 958423936.000000  [ 1344/ 1460]\n",
            "Epoch 66\n",
            "-------------------------------\n",
            "loss: 741706496.000000  [   64/ 1460]\n",
            "loss: 732772736.000000  [  704/ 1460]\n",
            "loss: 1191367936.000000  [ 1344/ 1460]\n",
            "Epoch 67\n",
            "-------------------------------\n",
            "loss: 761802560.000000  [   64/ 1460]\n",
            "loss: 801973952.000000  [  704/ 1460]\n",
            "loss: 672104704.000000  [ 1344/ 1460]\n",
            "Epoch 68\n",
            "-------------------------------\n",
            "loss: 691330560.000000  [   64/ 1460]\n",
            "loss: 763267968.000000  [  704/ 1460]\n",
            "loss: 608848960.000000  [ 1344/ 1460]\n",
            "Epoch 69\n",
            "-------------------------------\n",
            "loss: 591268480.000000  [   64/ 1460]\n",
            "loss: 1049302784.000000  [  704/ 1460]\n",
            "loss: 1177291648.000000  [ 1344/ 1460]\n",
            "Epoch 70\n",
            "-------------------------------\n",
            "loss: 724466688.000000  [   64/ 1460]\n",
            "loss: 474649920.000000  [  704/ 1460]\n",
            "loss: 787105280.000000  [ 1344/ 1460]\n",
            "Epoch 71\n",
            "-------------------------------\n",
            "loss: 1399407616.000000  [   64/ 1460]\n",
            "loss: 597093312.000000  [  704/ 1460]\n",
            "loss: 1414511616.000000  [ 1344/ 1460]\n",
            "Epoch 72\n",
            "-------------------------------\n",
            "loss: 753357376.000000  [   64/ 1460]\n",
            "loss: 615803456.000000  [  704/ 1460]\n",
            "loss: 756837376.000000  [ 1344/ 1460]\n",
            "Epoch 73\n",
            "-------------------------------\n",
            "loss: 951161728.000000  [   64/ 1460]\n",
            "loss: 1382136064.000000  [  704/ 1460]\n",
            "loss: 817420416.000000  [ 1344/ 1460]\n",
            "Epoch 74\n",
            "-------------------------------\n",
            "loss: 641576064.000000  [   64/ 1460]\n",
            "loss: 577731712.000000  [  704/ 1460]\n",
            "loss: 632731520.000000  [ 1344/ 1460]\n",
            "Epoch 75\n",
            "-------------------------------\n",
            "loss: 892302336.000000  [   64/ 1460]\n",
            "loss: 610499520.000000  [  704/ 1460]\n",
            "loss: 947228416.000000  [ 1344/ 1460]\n",
            "Epoch 76\n",
            "-------------------------------\n",
            "loss: 1518716416.000000  [   64/ 1460]\n",
            "loss: 789846080.000000  [  704/ 1460]\n",
            "loss: 906340736.000000  [ 1344/ 1460]\n",
            "Epoch 77\n",
            "-------------------------------\n",
            "loss: 462533504.000000  [   64/ 1460]\n",
            "loss: 1236642944.000000  [  704/ 1460]\n",
            "loss: 1156713216.000000  [ 1344/ 1460]\n",
            "Epoch 78\n",
            "-------------------------------\n",
            "loss: 1238128256.000000  [   64/ 1460]\n",
            "loss: 711774464.000000  [  704/ 1460]\n",
            "loss: 947476480.000000  [ 1344/ 1460]\n",
            "Epoch 79\n",
            "-------------------------------\n",
            "loss: 816521536.000000  [   64/ 1460]\n",
            "loss: 982406336.000000  [  704/ 1460]\n",
            "loss: 585640896.000000  [ 1344/ 1460]\n",
            "Epoch 80\n",
            "-------------------------------\n",
            "loss: 619920832.000000  [   64/ 1460]\n",
            "loss: 900687040.000000  [  704/ 1460]\n",
            "loss: 718748416.000000  [ 1344/ 1460]\n",
            "Epoch 81\n",
            "-------------------------------\n",
            "loss: 540275456.000000  [   64/ 1460]\n",
            "loss: 943310528.000000  [  704/ 1460]\n",
            "loss: 783766976.000000  [ 1344/ 1460]\n",
            "Epoch 82\n",
            "-------------------------------\n",
            "loss: 909978816.000000  [   64/ 1460]\n",
            "loss: 550472320.000000  [  704/ 1460]\n",
            "loss: 808795008.000000  [ 1344/ 1460]\n",
            "Epoch 83\n",
            "-------------------------------\n",
            "loss: 1831409152.000000  [   64/ 1460]\n",
            "loss: 776137664.000000  [  704/ 1460]\n",
            "loss: 1504031744.000000  [ 1344/ 1460]\n",
            "Epoch 84\n",
            "-------------------------------\n",
            "loss: 819373440.000000  [   64/ 1460]\n",
            "loss: 1410724608.000000  [  704/ 1460]\n",
            "loss: 1144937984.000000  [ 1344/ 1460]\n",
            "Epoch 85\n",
            "-------------------------------\n",
            "loss: 1291331840.000000  [   64/ 1460]\n",
            "loss: 1176186240.000000  [  704/ 1460]\n",
            "loss: 814893888.000000  [ 1344/ 1460]\n",
            "Epoch 86\n",
            "-------------------------------\n",
            "loss: 2314971648.000000  [   64/ 1460]\n",
            "loss: 1049095936.000000  [  704/ 1460]\n",
            "loss: 767936832.000000  [ 1344/ 1460]\n",
            "Epoch 87\n",
            "-------------------------------\n",
            "loss: 892644096.000000  [   64/ 1460]\n",
            "loss: 789320128.000000  [  704/ 1460]\n",
            "loss: 812596736.000000  [ 1344/ 1460]\n",
            "Epoch 88\n",
            "-------------------------------\n",
            "loss: 558564992.000000  [   64/ 1460]\n",
            "loss: 1295378432.000000  [  704/ 1460]\n",
            "loss: 1315293312.000000  [ 1344/ 1460]\n",
            "Epoch 89\n",
            "-------------------------------\n",
            "loss: 714904256.000000  [   64/ 1460]\n",
            "loss: 1420286848.000000  [  704/ 1460]\n",
            "loss: 633936704.000000  [ 1344/ 1460]\n",
            "Epoch 90\n",
            "-------------------------------\n",
            "loss: 861941888.000000  [   64/ 1460]\n",
            "loss: 676185856.000000  [  704/ 1460]\n",
            "loss: 1568820736.000000  [ 1344/ 1460]\n",
            "Epoch 91\n",
            "-------------------------------\n",
            "loss: 822195072.000000  [   64/ 1460]\n",
            "loss: 600107456.000000  [  704/ 1460]\n",
            "loss: 684026944.000000  [ 1344/ 1460]\n",
            "Epoch 92\n",
            "-------------------------------\n",
            "loss: 1244337280.000000  [   64/ 1460]\n",
            "loss: 974035776.000000  [  704/ 1460]\n",
            "loss: 1170112512.000000  [ 1344/ 1460]\n",
            "Epoch 93\n",
            "-------------------------------\n",
            "loss: 992576512.000000  [   64/ 1460]\n",
            "loss: 775860224.000000  [  704/ 1460]\n",
            "loss: 1835873792.000000  [ 1344/ 1460]\n",
            "Epoch 94\n",
            "-------------------------------\n",
            "loss: 976466304.000000  [   64/ 1460]\n",
            "loss: 836653952.000000  [  704/ 1460]\n",
            "loss: 1035927040.000000  [ 1344/ 1460]\n",
            "Epoch 95\n",
            "-------------------------------\n",
            "loss: 877169344.000000  [   64/ 1460]\n",
            "loss: 794883200.000000  [  704/ 1460]\n",
            "loss: 963883200.000000  [ 1344/ 1460]\n",
            "Epoch 96\n",
            "-------------------------------\n",
            "loss: 546933952.000000  [   64/ 1460]\n",
            "loss: 1030111872.000000  [  704/ 1460]\n",
            "loss: 916918528.000000  [ 1344/ 1460]\n",
            "Epoch 97\n",
            "-------------------------------\n",
            "loss: 396725888.000000  [   64/ 1460]\n",
            "loss: 903383552.000000  [  704/ 1460]\n",
            "loss: 2964922880.000000  [ 1344/ 1460]\n",
            "Epoch 98\n",
            "-------------------------------\n",
            "loss: 1655422464.000000  [   64/ 1460]\n",
            "loss: 1040829696.000000  [  704/ 1460]\n",
            "loss: 1286204160.000000  [ 1344/ 1460]\n",
            "Epoch 99\n",
            "-------------------------------\n",
            "loss: 878786944.000000  [   64/ 1460]\n",
            "loss: 684845248.000000  [  704/ 1460]\n",
            "loss: 967204736.000000  [ 1344/ 1460]\n",
            "Epoch 100\n",
            "-------------------------------\n",
            "loss: 1124759808.000000  [   64/ 1460]\n",
            "loss: 702567936.000000  [  704/ 1460]\n",
            "loss: 440296192.000000  [ 1344/ 1460]\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_features = test_features.to(device)\n",
        "\n",
        "model = torch.load('/content/drive/MyDrive/checkpoint/model.pth')\n",
        "model.eval()\n",
        "\n",
        "\n",
        "preds = model(test_features).detach().numpy()\n",
        "# 将其重新格式化以导出到Kaggle\n",
        "test_data['SalePrice'] = pd.Series(preds.reshape(-1, 1)[:,0])\n",
        "submission = pd.concat([test_data['Id'], test_data['SalePrice']], axis=1)\n",
        "submission.to_csv('submission.csv', index=False)"
      ],
      "metadata": {
        "id": "qZ8NftqDCBm5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oEmY4pRuT3Ta"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}